{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 30.0,
  "eval_steps": 500,
  "global_step": 136920,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.10955302366345311,
      "grad_norm": 4.752491474151611,
      "learning_rate": 4.981741162722758e-05,
      "loss": 1.823,
      "step": 500
    },
    {
      "epoch": 0.21910604732690622,
      "grad_norm": 7.985696792602539,
      "learning_rate": 4.963482325445516e-05,
      "loss": 1.808,
      "step": 1000
    },
    {
      "epoch": 0.32865907099035935,
      "grad_norm": 4.275625228881836,
      "learning_rate": 4.9452234881682734e-05,
      "loss": 1.8016,
      "step": 1500
    },
    {
      "epoch": 0.43821209465381245,
      "grad_norm": 5.474124908447266,
      "learning_rate": 4.9269646508910316e-05,
      "loss": 1.8071,
      "step": 2000
    },
    {
      "epoch": 0.5477651183172656,
      "grad_norm": 7.447843074798584,
      "learning_rate": 4.908705813613789e-05,
      "loss": 1.803,
      "step": 2500
    },
    {
      "epoch": 0.6573181419807187,
      "grad_norm": 6.175232887268066,
      "learning_rate": 4.890446976336547e-05,
      "loss": 1.7866,
      "step": 3000
    },
    {
      "epoch": 0.7668711656441718,
      "grad_norm": 6.558139324188232,
      "learning_rate": 4.872188139059305e-05,
      "loss": 1.7806,
      "step": 3500
    },
    {
      "epoch": 0.8764241893076249,
      "grad_norm": 5.900414943695068,
      "learning_rate": 4.853929301782062e-05,
      "loss": 1.8018,
      "step": 4000
    },
    {
      "epoch": 0.985977212971078,
      "grad_norm": 4.953217506408691,
      "learning_rate": 4.8356704645048205e-05,
      "loss": 1.7901,
      "step": 4500
    },
    {
      "epoch": 1.0955302366345312,
      "grad_norm": 9.088863372802734,
      "learning_rate": 4.817411627227578e-05,
      "loss": 1.7836,
      "step": 5000
    },
    {
      "epoch": 1.2050832602979842,
      "grad_norm": 6.351284503936768,
      "learning_rate": 4.799152789950336e-05,
      "loss": 1.7803,
      "step": 5500
    },
    {
      "epoch": 1.3146362839614374,
      "grad_norm": 5.908259391784668,
      "learning_rate": 4.780893952673094e-05,
      "loss": 1.7803,
      "step": 6000
    },
    {
      "epoch": 1.4241893076248904,
      "grad_norm": 5.0944061279296875,
      "learning_rate": 4.762635115395852e-05,
      "loss": 1.7913,
      "step": 6500
    },
    {
      "epoch": 1.5337423312883436,
      "grad_norm": 7.535539627075195,
      "learning_rate": 4.7443762781186094e-05,
      "loss": 1.7776,
      "step": 7000
    },
    {
      "epoch": 1.6432953549517966,
      "grad_norm": 5.103445053100586,
      "learning_rate": 4.726117440841367e-05,
      "loss": 1.7752,
      "step": 7500
    },
    {
      "epoch": 1.7528483786152498,
      "grad_norm": 6.701585292816162,
      "learning_rate": 4.707858603564125e-05,
      "loss": 1.7746,
      "step": 8000
    },
    {
      "epoch": 1.862401402278703,
      "grad_norm": 6.325949192047119,
      "learning_rate": 4.6895997662868826e-05,
      "loss": 1.7839,
      "step": 8500
    },
    {
      "epoch": 1.971954425942156,
      "grad_norm": 6.02607536315918,
      "learning_rate": 4.671340929009641e-05,
      "loss": 1.7825,
      "step": 9000
    },
    {
      "epoch": 2.081507449605609,
      "grad_norm": 7.038148403167725,
      "learning_rate": 4.653082091732398e-05,
      "loss": 1.7904,
      "step": 9500
    },
    {
      "epoch": 2.1910604732690624,
      "grad_norm": 5.809728145599365,
      "learning_rate": 4.6348232544551565e-05,
      "loss": 1.8071,
      "step": 10000
    },
    {
      "epoch": 2.3006134969325154,
      "grad_norm": 4.111434459686279,
      "learning_rate": 4.616564417177914e-05,
      "loss": 1.7923,
      "step": 10500
    },
    {
      "epoch": 2.4101665205959684,
      "grad_norm": 4.317472457885742,
      "learning_rate": 4.598305579900672e-05,
      "loss": 1.7969,
      "step": 11000
    },
    {
      "epoch": 2.519719544259422,
      "grad_norm": 4.682844161987305,
      "learning_rate": 4.5800467426234304e-05,
      "loss": 1.8431,
      "step": 11500
    },
    {
      "epoch": 2.629272567922875,
      "grad_norm": 4.410357475280762,
      "learning_rate": 4.561787905346188e-05,
      "loss": 1.8494,
      "step": 12000
    },
    {
      "epoch": 2.7388255915863278,
      "grad_norm": 7.942983150482178,
      "learning_rate": 4.543529068068946e-05,
      "loss": 1.8512,
      "step": 12500
    },
    {
      "epoch": 2.8483786152497808,
      "grad_norm": 5.047688961029053,
      "learning_rate": 4.5252702307917036e-05,
      "loss": 1.8407,
      "step": 13000
    },
    {
      "epoch": 2.9579316389132337,
      "grad_norm": 4.8019185066223145,
      "learning_rate": 4.507011393514462e-05,
      "loss": 1.8402,
      "step": 13500
    },
    {
      "epoch": 3.067484662576687,
      "grad_norm": 8.771296501159668,
      "learning_rate": 4.488752556237219e-05,
      "loss": 1.852,
      "step": 14000
    },
    {
      "epoch": 3.17703768624014,
      "grad_norm": 5.135885238647461,
      "learning_rate": 4.470493718959977e-05,
      "loss": 1.8382,
      "step": 14500
    },
    {
      "epoch": 3.286590709903593,
      "grad_norm": 5.57983922958374,
      "learning_rate": 4.452234881682735e-05,
      "loss": 1.854,
      "step": 15000
    },
    {
      "epoch": 3.3961437335670466,
      "grad_norm": 9.919718742370605,
      "learning_rate": 4.4339760444054924e-05,
      "loss": 1.8459,
      "step": 15500
    },
    {
      "epoch": 3.5056967572304996,
      "grad_norm": 4.135495185852051,
      "learning_rate": 4.4157172071282506e-05,
      "loss": 1.8466,
      "step": 16000
    },
    {
      "epoch": 3.6152497808939525,
      "grad_norm": 6.112337589263916,
      "learning_rate": 4.397458369851008e-05,
      "loss": 1.8538,
      "step": 16500
    },
    {
      "epoch": 3.724802804557406,
      "grad_norm": 5.743913650512695,
      "learning_rate": 4.379199532573766e-05,
      "loss": 1.8439,
      "step": 17000
    },
    {
      "epoch": 3.834355828220859,
      "grad_norm": 5.443830966949463,
      "learning_rate": 4.360940695296524e-05,
      "loss": 1.8486,
      "step": 17500
    },
    {
      "epoch": 3.943908851884312,
      "grad_norm": 6.112320423126221,
      "learning_rate": 4.3426818580192813e-05,
      "loss": 1.8419,
      "step": 18000
    },
    {
      "epoch": 4.053461875547765,
      "grad_norm": 5.332447528839111,
      "learning_rate": 4.3244230207420395e-05,
      "loss": 1.8357,
      "step": 18500
    },
    {
      "epoch": 4.163014899211218,
      "grad_norm": 5.420506954193115,
      "learning_rate": 4.306164183464797e-05,
      "loss": 1.8477,
      "step": 19000
    },
    {
      "epoch": 4.272567922874671,
      "grad_norm": 5.560032367706299,
      "learning_rate": 4.287905346187555e-05,
      "loss": 1.8488,
      "step": 19500
    },
    {
      "epoch": 4.382120946538125,
      "grad_norm": 7.3883376121521,
      "learning_rate": 4.269646508910313e-05,
      "loss": 1.8499,
      "step": 20000
    },
    {
      "epoch": 4.491673970201577,
      "grad_norm": 5.040181636810303,
      "learning_rate": 4.251387671633071e-05,
      "loss": 1.8438,
      "step": 20500
    },
    {
      "epoch": 4.601226993865031,
      "grad_norm": 8.362051010131836,
      "learning_rate": 4.2331288343558284e-05,
      "loss": 1.8416,
      "step": 21000
    },
    {
      "epoch": 4.710780017528483,
      "grad_norm": 7.15593957901001,
      "learning_rate": 4.214869997078586e-05,
      "loss": 1.8473,
      "step": 21500
    },
    {
      "epoch": 4.820333041191937,
      "grad_norm": 7.14856481552124,
      "learning_rate": 4.196611159801344e-05,
      "loss": 1.8371,
      "step": 22000
    },
    {
      "epoch": 4.92988606485539,
      "grad_norm": 6.776979446411133,
      "learning_rate": 4.1783523225241016e-05,
      "loss": 1.8396,
      "step": 22500
    },
    {
      "epoch": 5.039439088518843,
      "grad_norm": 6.706996917724609,
      "learning_rate": 4.16009348524686e-05,
      "loss": 1.8462,
      "step": 23000
    },
    {
      "epoch": 5.148992112182296,
      "grad_norm": 3.568876266479492,
      "learning_rate": 4.141834647969617e-05,
      "loss": 1.8477,
      "step": 23500
    },
    {
      "epoch": 5.25854513584575,
      "grad_norm": 2.863088846206665,
      "learning_rate": 4.1235758106923755e-05,
      "loss": 1.8472,
      "step": 24000
    },
    {
      "epoch": 5.368098159509202,
      "grad_norm": 5.715033531188965,
      "learning_rate": 4.105316973415133e-05,
      "loss": 1.8381,
      "step": 24500
    },
    {
      "epoch": 5.4776511831726555,
      "grad_norm": 3.9219672679901123,
      "learning_rate": 4.0870581361378905e-05,
      "loss": 1.8526,
      "step": 25000
    },
    {
      "epoch": 5.587204206836109,
      "grad_norm": 6.675719738006592,
      "learning_rate": 4.068799298860649e-05,
      "loss": 1.8446,
      "step": 25500
    },
    {
      "epoch": 5.6967572304995615,
      "grad_norm": 4.501495361328125,
      "learning_rate": 4.050540461583406e-05,
      "loss": 1.8334,
      "step": 26000
    },
    {
      "epoch": 5.806310254163015,
      "grad_norm": 6.377885818481445,
      "learning_rate": 4.0322816243061644e-05,
      "loss": 1.841,
      "step": 26500
    },
    {
      "epoch": 5.915863277826468,
      "grad_norm": 5.232044219970703,
      "learning_rate": 4.014022787028922e-05,
      "loss": 1.8477,
      "step": 27000
    },
    {
      "epoch": 6.025416301489921,
      "grad_norm": 7.577013969421387,
      "learning_rate": 3.99576394975168e-05,
      "loss": 1.8419,
      "step": 27500
    },
    {
      "epoch": 6.134969325153374,
      "grad_norm": 8.643619537353516,
      "learning_rate": 3.9775051124744376e-05,
      "loss": 1.8398,
      "step": 28000
    },
    {
      "epoch": 6.244522348816828,
      "grad_norm": 4.63752555847168,
      "learning_rate": 3.959246275197195e-05,
      "loss": 1.8464,
      "step": 28500
    },
    {
      "epoch": 6.35407537248028,
      "grad_norm": 6.422142028808594,
      "learning_rate": 3.940987437919953e-05,
      "loss": 1.8399,
      "step": 29000
    },
    {
      "epoch": 6.463628396143734,
      "grad_norm": 5.914055347442627,
      "learning_rate": 3.922728600642711e-05,
      "loss": 1.8505,
      "step": 29500
    },
    {
      "epoch": 6.573181419807186,
      "grad_norm": 8.662493705749512,
      "learning_rate": 3.904469763365469e-05,
      "loss": 1.8439,
      "step": 30000
    },
    {
      "epoch": 6.68273444347064,
      "grad_norm": 5.5642523765563965,
      "learning_rate": 3.8862109260882265e-05,
      "loss": 1.8359,
      "step": 30500
    },
    {
      "epoch": 6.792287467134093,
      "grad_norm": 3.2756221294403076,
      "learning_rate": 3.8679520888109847e-05,
      "loss": 1.8447,
      "step": 31000
    },
    {
      "epoch": 6.901840490797546,
      "grad_norm": 9.546642303466797,
      "learning_rate": 3.849693251533742e-05,
      "loss": 1.8465,
      "step": 31500
    },
    {
      "epoch": 7.011393514460999,
      "grad_norm": 3.9819648265838623,
      "learning_rate": 3.8314344142565003e-05,
      "loss": 1.8342,
      "step": 32000
    },
    {
      "epoch": 7.1209465381244526,
      "grad_norm": 4.819598197937012,
      "learning_rate": 3.8131755769792585e-05,
      "loss": 1.8444,
      "step": 32500
    },
    {
      "epoch": 7.230499561787905,
      "grad_norm": 6.097964763641357,
      "learning_rate": 3.794916739702016e-05,
      "loss": 1.8327,
      "step": 33000
    },
    {
      "epoch": 7.3400525854513585,
      "grad_norm": 7.080332279205322,
      "learning_rate": 3.776657902424774e-05,
      "loss": 1.841,
      "step": 33500
    },
    {
      "epoch": 7.449605609114812,
      "grad_norm": 4.197520732879639,
      "learning_rate": 3.758399065147532e-05,
      "loss": 1.8327,
      "step": 34000
    },
    {
      "epoch": 7.5591586327782645,
      "grad_norm": 3.341581344604492,
      "learning_rate": 3.74014022787029e-05,
      "loss": 1.8464,
      "step": 34500
    },
    {
      "epoch": 7.668711656441718,
      "grad_norm": 6.934189796447754,
      "learning_rate": 3.7218813905930474e-05,
      "loss": 1.8445,
      "step": 35000
    },
    {
      "epoch": 7.7782646801051705,
      "grad_norm": 6.672372341156006,
      "learning_rate": 3.703622553315805e-05,
      "loss": 1.8389,
      "step": 35500
    },
    {
      "epoch": 7.887817703768624,
      "grad_norm": 5.952651500701904,
      "learning_rate": 3.685363716038563e-05,
      "loss": 1.8488,
      "step": 36000
    },
    {
      "epoch": 7.997370727432077,
      "grad_norm": 4.398970127105713,
      "learning_rate": 3.6671048787613206e-05,
      "loss": 1.8463,
      "step": 36500
    },
    {
      "epoch": 8.10692375109553,
      "grad_norm": 4.633565425872803,
      "learning_rate": 3.648846041484079e-05,
      "loss": 1.8407,
      "step": 37000
    },
    {
      "epoch": 8.216476774758984,
      "grad_norm": 3.9884846210479736,
      "learning_rate": 3.630587204206836e-05,
      "loss": 1.8442,
      "step": 37500
    },
    {
      "epoch": 8.326029798422436,
      "grad_norm": 2.4221010208129883,
      "learning_rate": 3.6123283669295945e-05,
      "loss": 1.8369,
      "step": 38000
    },
    {
      "epoch": 8.43558282208589,
      "grad_norm": 4.708592891693115,
      "learning_rate": 3.594069529652352e-05,
      "loss": 1.8413,
      "step": 38500
    },
    {
      "epoch": 8.545135845749343,
      "grad_norm": 7.314738750457764,
      "learning_rate": 3.5758106923751095e-05,
      "loss": 1.8419,
      "step": 39000
    },
    {
      "epoch": 8.654688869412796,
      "grad_norm": 6.527626991271973,
      "learning_rate": 3.557551855097868e-05,
      "loss": 1.8458,
      "step": 39500
    },
    {
      "epoch": 8.76424189307625,
      "grad_norm": 7.472295761108398,
      "learning_rate": 3.539293017820625e-05,
      "loss": 1.8377,
      "step": 40000
    },
    {
      "epoch": 8.873794916739701,
      "grad_norm": 7.179033279418945,
      "learning_rate": 3.5210341805433834e-05,
      "loss": 1.8445,
      "step": 40500
    },
    {
      "epoch": 8.983347940403155,
      "grad_norm": 8.731390953063965,
      "learning_rate": 3.502775343266141e-05,
      "loss": 1.8431,
      "step": 41000
    },
    {
      "epoch": 9.092900964066608,
      "grad_norm": 3.782310962677002,
      "learning_rate": 3.484516505988899e-05,
      "loss": 1.8362,
      "step": 41500
    },
    {
      "epoch": 9.202453987730062,
      "grad_norm": 7.722362518310547,
      "learning_rate": 3.4662576687116566e-05,
      "loss": 1.8372,
      "step": 42000
    },
    {
      "epoch": 9.312007011393515,
      "grad_norm": 3.5471904277801514,
      "learning_rate": 3.447998831434414e-05,
      "loss": 1.834,
      "step": 42500
    },
    {
      "epoch": 9.421560035056968,
      "grad_norm": 3.9491043090820312,
      "learning_rate": 3.429739994157172e-05,
      "loss": 1.8399,
      "step": 43000
    },
    {
      "epoch": 9.53111305872042,
      "grad_norm": 4.743711471557617,
      "learning_rate": 3.41148115687993e-05,
      "loss": 1.8369,
      "step": 43500
    },
    {
      "epoch": 9.640666082383873,
      "grad_norm": 6.403232574462891,
      "learning_rate": 3.393222319602688e-05,
      "loss": 1.8451,
      "step": 44000
    },
    {
      "epoch": 9.750219106047327,
      "grad_norm": 4.554828643798828,
      "learning_rate": 3.3749634823254455e-05,
      "loss": 1.8373,
      "step": 44500
    },
    {
      "epoch": 9.85977212971078,
      "grad_norm": 3.9462060928344727,
      "learning_rate": 3.356704645048204e-05,
      "loss": 1.8399,
      "step": 45000
    },
    {
      "epoch": 9.969325153374234,
      "grad_norm": 4.392104148864746,
      "learning_rate": 3.338445807770961e-05,
      "loss": 1.853,
      "step": 45500
    },
    {
      "epoch": 10.078878177037685,
      "grad_norm": 7.776413917541504,
      "learning_rate": 3.320186970493719e-05,
      "loss": 1.8429,
      "step": 46000
    },
    {
      "epoch": 10.188431200701139,
      "grad_norm": 7.86297607421875,
      "learning_rate": 3.301928133216477e-05,
      "loss": 1.8413,
      "step": 46500
    },
    {
      "epoch": 10.297984224364592,
      "grad_norm": 6.289257049560547,
      "learning_rate": 3.2836692959392344e-05,
      "loss": 1.8393,
      "step": 47000
    },
    {
      "epoch": 10.407537248028046,
      "grad_norm": 7.001659393310547,
      "learning_rate": 3.2654104586619926e-05,
      "loss": 1.8438,
      "step": 47500
    },
    {
      "epoch": 10.5170902716915,
      "grad_norm": 8.419854164123535,
      "learning_rate": 3.24715162138475e-05,
      "loss": 1.8422,
      "step": 48000
    },
    {
      "epoch": 10.626643295354953,
      "grad_norm": 7.4520087242126465,
      "learning_rate": 3.228892784107508e-05,
      "loss": 1.8371,
      "step": 48500
    },
    {
      "epoch": 10.736196319018404,
      "grad_norm": 8.449159622192383,
      "learning_rate": 3.210633946830266e-05,
      "loss": 1.8389,
      "step": 49000
    },
    {
      "epoch": 10.845749342681858,
      "grad_norm": 7.146060943603516,
      "learning_rate": 3.192375109553023e-05,
      "loss": 1.8364,
      "step": 49500
    },
    {
      "epoch": 10.955302366345311,
      "grad_norm": 7.271997451782227,
      "learning_rate": 3.1741162722757814e-05,
      "loss": 1.8438,
      "step": 50000
    },
    {
      "epoch": 11.064855390008765,
      "grad_norm": 4.065558433532715,
      "learning_rate": 3.155857434998539e-05,
      "loss": 1.8336,
      "step": 50500
    },
    {
      "epoch": 11.174408413672218,
      "grad_norm": 5.617664813995361,
      "learning_rate": 3.137598597721297e-05,
      "loss": 1.8391,
      "step": 51000
    },
    {
      "epoch": 11.28396143733567,
      "grad_norm": 4.300732612609863,
      "learning_rate": 3.1193397604440546e-05,
      "loss": 1.8369,
      "step": 51500
    },
    {
      "epoch": 11.393514460999123,
      "grad_norm": 8.297987937927246,
      "learning_rate": 3.101080923166813e-05,
      "loss": 1.8438,
      "step": 52000
    },
    {
      "epoch": 11.503067484662576,
      "grad_norm": 4.682804107666016,
      "learning_rate": 3.0828220858895703e-05,
      "loss": 1.8389,
      "step": 52500
    },
    {
      "epoch": 11.61262050832603,
      "grad_norm": 5.427230358123779,
      "learning_rate": 3.0645632486123285e-05,
      "loss": 1.8483,
      "step": 53000
    },
    {
      "epoch": 11.722173531989483,
      "grad_norm": 4.655004978179932,
      "learning_rate": 3.0463044113350864e-05,
      "loss": 1.8464,
      "step": 53500
    },
    {
      "epoch": 11.831726555652937,
      "grad_norm": 6.584589958190918,
      "learning_rate": 3.028045574057844e-05,
      "loss": 1.8338,
      "step": 54000
    },
    {
      "epoch": 11.941279579316388,
      "grad_norm": 5.454513072967529,
      "learning_rate": 3.009786736780602e-05,
      "loss": 1.8333,
      "step": 54500
    },
    {
      "epoch": 12.050832602979842,
      "grad_norm": 7.2732696533203125,
      "learning_rate": 2.9915278995033596e-05,
      "loss": 1.8389,
      "step": 55000
    },
    {
      "epoch": 12.160385626643295,
      "grad_norm": 8.122432708740234,
      "learning_rate": 2.9732690622261178e-05,
      "loss": 1.84,
      "step": 55500
    },
    {
      "epoch": 12.269938650306749,
      "grad_norm": 8.88217544555664,
      "learning_rate": 2.9550102249488753e-05,
      "loss": 1.8459,
      "step": 56000
    },
    {
      "epoch": 12.379491673970202,
      "grad_norm": 5.432063102722168,
      "learning_rate": 2.9367513876716328e-05,
      "loss": 1.8356,
      "step": 56500
    },
    {
      "epoch": 12.489044697633656,
      "grad_norm": 7.070793151855469,
      "learning_rate": 2.918492550394391e-05,
      "loss": 1.8442,
      "step": 57000
    },
    {
      "epoch": 12.598597721297107,
      "grad_norm": 8.217334747314453,
      "learning_rate": 2.9002337131171488e-05,
      "loss": 1.8495,
      "step": 57500
    },
    {
      "epoch": 12.70815074496056,
      "grad_norm": 9.55253791809082,
      "learning_rate": 2.8819748758399066e-05,
      "loss": 1.8372,
      "step": 58000
    },
    {
      "epoch": 12.817703768624014,
      "grad_norm": 5.016307830810547,
      "learning_rate": 2.8637160385626645e-05,
      "loss": 1.8395,
      "step": 58500
    },
    {
      "epoch": 12.927256792287467,
      "grad_norm": 2.3025662899017334,
      "learning_rate": 2.8454572012854223e-05,
      "loss": 1.8366,
      "step": 59000
    },
    {
      "epoch": 13.036809815950921,
      "grad_norm": 7.292879104614258,
      "learning_rate": 2.8271983640081802e-05,
      "loss": 1.8351,
      "step": 59500
    },
    {
      "epoch": 13.146362839614373,
      "grad_norm": 6.7764434814453125,
      "learning_rate": 2.8089395267309377e-05,
      "loss": 1.84,
      "step": 60000
    },
    {
      "epoch": 13.255915863277826,
      "grad_norm": 5.507358074188232,
      "learning_rate": 2.790680689453696e-05,
      "loss": 1.8415,
      "step": 60500
    },
    {
      "epoch": 13.36546888694128,
      "grad_norm": 7.204901695251465,
      "learning_rate": 2.7724218521764534e-05,
      "loss": 1.8414,
      "step": 61000
    },
    {
      "epoch": 13.475021910604733,
      "grad_norm": 6.584852695465088,
      "learning_rate": 2.7541630148992116e-05,
      "loss": 1.8356,
      "step": 61500
    },
    {
      "epoch": 13.584574934268186,
      "grad_norm": 4.77722692489624,
      "learning_rate": 2.735904177621969e-05,
      "loss": 1.8364,
      "step": 62000
    },
    {
      "epoch": 13.69412795793164,
      "grad_norm": 5.26575231552124,
      "learning_rate": 2.7176453403447273e-05,
      "loss": 1.841,
      "step": 62500
    },
    {
      "epoch": 13.803680981595091,
      "grad_norm": 7.748274326324463,
      "learning_rate": 2.6993865030674848e-05,
      "loss": 1.8358,
      "step": 63000
    },
    {
      "epoch": 13.913234005258545,
      "grad_norm": 4.856236934661865,
      "learning_rate": 2.681127665790243e-05,
      "loss": 1.8447,
      "step": 63500
    },
    {
      "epoch": 14.022787028921998,
      "grad_norm": 7.480156421661377,
      "learning_rate": 2.6628688285130005e-05,
      "loss": 1.8391,
      "step": 64000
    },
    {
      "epoch": 14.132340052585452,
      "grad_norm": 3.551208019256592,
      "learning_rate": 2.644609991235758e-05,
      "loss": 1.8403,
      "step": 64500
    },
    {
      "epoch": 14.241893076248905,
      "grad_norm": 4.959095001220703,
      "learning_rate": 2.626351153958516e-05,
      "loss": 1.8428,
      "step": 65000
    },
    {
      "epoch": 14.351446099912357,
      "grad_norm": 5.979523181915283,
      "learning_rate": 2.6080923166812737e-05,
      "loss": 1.8321,
      "step": 65500
    },
    {
      "epoch": 14.46099912357581,
      "grad_norm": 6.035109043121338,
      "learning_rate": 2.589833479404032e-05,
      "loss": 1.8362,
      "step": 66000
    },
    {
      "epoch": 14.570552147239264,
      "grad_norm": 7.4100117683410645,
      "learning_rate": 2.5715746421267894e-05,
      "loss": 1.8408,
      "step": 66500
    },
    {
      "epoch": 14.680105170902717,
      "grad_norm": 3.570322275161743,
      "learning_rate": 2.5533158048495475e-05,
      "loss": 1.84,
      "step": 67000
    },
    {
      "epoch": 14.78965819456617,
      "grad_norm": 11.158957481384277,
      "learning_rate": 2.535056967572305e-05,
      "loss": 1.844,
      "step": 67500
    },
    {
      "epoch": 14.899211218229624,
      "grad_norm": 6.428901195526123,
      "learning_rate": 2.516798130295063e-05,
      "loss": 1.8357,
      "step": 68000
    },
    {
      "epoch": 15.008764241893076,
      "grad_norm": 4.736730575561523,
      "learning_rate": 2.4985392930178207e-05,
      "loss": 1.8324,
      "step": 68500
    },
    {
      "epoch": 15.118317265556529,
      "grad_norm": 3.5624570846557617,
      "learning_rate": 2.4802804557405786e-05,
      "loss": 1.843,
      "step": 69000
    },
    {
      "epoch": 15.227870289219982,
      "grad_norm": 5.202145099639893,
      "learning_rate": 2.4620216184633364e-05,
      "loss": 1.8487,
      "step": 69500
    },
    {
      "epoch": 15.337423312883436,
      "grad_norm": 6.0962934494018555,
      "learning_rate": 2.4437627811860943e-05,
      "loss": 1.8349,
      "step": 70000
    },
    {
      "epoch": 15.44697633654689,
      "grad_norm": 6.073258876800537,
      "learning_rate": 2.425503943908852e-05,
      "loss": 1.8365,
      "step": 70500
    },
    {
      "epoch": 15.556529360210341,
      "grad_norm": 5.582208156585693,
      "learning_rate": 2.40724510663161e-05,
      "loss": 1.8344,
      "step": 71000
    },
    {
      "epoch": 15.666082383873794,
      "grad_norm": 8.390241622924805,
      "learning_rate": 2.3889862693543678e-05,
      "loss": 1.8389,
      "step": 71500
    },
    {
      "epoch": 15.775635407537248,
      "grad_norm": 6.208846569061279,
      "learning_rate": 2.3707274320771257e-05,
      "loss": 1.8323,
      "step": 72000
    },
    {
      "epoch": 15.885188431200701,
      "grad_norm": 4.162437438964844,
      "learning_rate": 2.352468594799883e-05,
      "loss": 1.8349,
      "step": 72500
    },
    {
      "epoch": 15.994741454864155,
      "grad_norm": 6.076113700866699,
      "learning_rate": 2.334209757522641e-05,
      "loss": 1.8315,
      "step": 73000
    },
    {
      "epoch": 16.104294478527606,
      "grad_norm": 6.22158145904541,
      "learning_rate": 2.315950920245399e-05,
      "loss": 1.8409,
      "step": 73500
    },
    {
      "epoch": 16.21384750219106,
      "grad_norm": 7.7540788650512695,
      "learning_rate": 2.2976920829681567e-05,
      "loss": 1.8378,
      "step": 74000
    },
    {
      "epoch": 16.323400525854513,
      "grad_norm": 6.562533855438232,
      "learning_rate": 2.2794332456909145e-05,
      "loss": 1.8418,
      "step": 74500
    },
    {
      "epoch": 16.43295354951797,
      "grad_norm": 7.17087459564209,
      "learning_rate": 2.2611744084136724e-05,
      "loss": 1.8347,
      "step": 75000
    },
    {
      "epoch": 16.54250657318142,
      "grad_norm": 4.243653297424316,
      "learning_rate": 2.2429155711364302e-05,
      "loss": 1.8367,
      "step": 75500
    },
    {
      "epoch": 16.65205959684487,
      "grad_norm": 8.863365173339844,
      "learning_rate": 2.2246567338591877e-05,
      "loss": 1.8357,
      "step": 76000
    },
    {
      "epoch": 16.761612620508327,
      "grad_norm": 4.834717273712158,
      "learning_rate": 2.2063978965819456e-05,
      "loss": 1.8323,
      "step": 76500
    },
    {
      "epoch": 16.87116564417178,
      "grad_norm": 5.009284973144531,
      "learning_rate": 2.1881390593047034e-05,
      "loss": 1.8395,
      "step": 77000
    },
    {
      "epoch": 16.980718667835234,
      "grad_norm": 4.137519836425781,
      "learning_rate": 2.1698802220274613e-05,
      "loss": 1.8384,
      "step": 77500
    },
    {
      "epoch": 17.090271691498685,
      "grad_norm": 7.163703441619873,
      "learning_rate": 2.151621384750219e-05,
      "loss": 1.836,
      "step": 78000
    },
    {
      "epoch": 17.199824715162137,
      "grad_norm": 4.310563087463379,
      "learning_rate": 2.133362547472977e-05,
      "loss": 1.8409,
      "step": 78500
    },
    {
      "epoch": 17.309377738825592,
      "grad_norm": 3.948326587677002,
      "learning_rate": 2.1151037101957348e-05,
      "loss": 1.8388,
      "step": 79000
    },
    {
      "epoch": 17.418930762489044,
      "grad_norm": 4.088114261627197,
      "learning_rate": 2.0968448729184927e-05,
      "loss": 1.8416,
      "step": 79500
    },
    {
      "epoch": 17.5284837861525,
      "grad_norm": 4.874014377593994,
      "learning_rate": 2.0785860356412505e-05,
      "loss": 1.8248,
      "step": 80000
    },
    {
      "epoch": 17.63803680981595,
      "grad_norm": 7.143665313720703,
      "learning_rate": 2.0603271983640084e-05,
      "loss": 1.8356,
      "step": 80500
    },
    {
      "epoch": 17.747589833479402,
      "grad_norm": 7.303609848022461,
      "learning_rate": 2.0420683610867662e-05,
      "loss": 1.8395,
      "step": 81000
    },
    {
      "epoch": 17.857142857142858,
      "grad_norm": 7.821459770202637,
      "learning_rate": 2.023809523809524e-05,
      "loss": 1.8321,
      "step": 81500
    },
    {
      "epoch": 17.96669588080631,
      "grad_norm": 6.092925071716309,
      "learning_rate": 2.005550686532282e-05,
      "loss": 1.8415,
      "step": 82000
    },
    {
      "epoch": 18.076248904469765,
      "grad_norm": 5.646001815795898,
      "learning_rate": 1.9872918492550397e-05,
      "loss": 1.835,
      "step": 82500
    },
    {
      "epoch": 18.185801928133216,
      "grad_norm": 3.1373348236083984,
      "learning_rate": 1.9690330119777973e-05,
      "loss": 1.8367,
      "step": 83000
    },
    {
      "epoch": 18.295354951796668,
      "grad_norm": 3.611964464187622,
      "learning_rate": 1.950774174700555e-05,
      "loss": 1.842,
      "step": 83500
    },
    {
      "epoch": 18.404907975460123,
      "grad_norm": 6.694332599639893,
      "learning_rate": 1.932515337423313e-05,
      "loss": 1.8285,
      "step": 84000
    },
    {
      "epoch": 18.514460999123575,
      "grad_norm": 6.89410400390625,
      "learning_rate": 1.9142565001460708e-05,
      "loss": 1.8464,
      "step": 84500
    },
    {
      "epoch": 18.62401402278703,
      "grad_norm": 4.469457626342773,
      "learning_rate": 1.8959976628688286e-05,
      "loss": 1.8322,
      "step": 85000
    },
    {
      "epoch": 18.73356704645048,
      "grad_norm": 6.691166877746582,
      "learning_rate": 1.8777388255915865e-05,
      "loss": 1.8347,
      "step": 85500
    },
    {
      "epoch": 18.843120070113937,
      "grad_norm": 8.32609748840332,
      "learning_rate": 1.8594799883143443e-05,
      "loss": 1.8367,
      "step": 86000
    },
    {
      "epoch": 18.95267309377739,
      "grad_norm": 7.495725154876709,
      "learning_rate": 1.841221151037102e-05,
      "loss": 1.841,
      "step": 86500
    },
    {
      "epoch": 19.06222611744084,
      "grad_norm": 3.7939321994781494,
      "learning_rate": 1.8229623137598597e-05,
      "loss": 1.8407,
      "step": 87000
    },
    {
      "epoch": 19.171779141104295,
      "grad_norm": 4.515763282775879,
      "learning_rate": 1.8047034764826175e-05,
      "loss": 1.8313,
      "step": 87500
    },
    {
      "epoch": 19.281332164767747,
      "grad_norm": 4.256186485290527,
      "learning_rate": 1.7864446392053754e-05,
      "loss": 1.8354,
      "step": 88000
    },
    {
      "epoch": 19.390885188431202,
      "grad_norm": 5.017545700073242,
      "learning_rate": 1.7681858019281332e-05,
      "loss": 1.8385,
      "step": 88500
    },
    {
      "epoch": 19.500438212094654,
      "grad_norm": 6.252902984619141,
      "learning_rate": 1.749926964650891e-05,
      "loss": 1.8352,
      "step": 89000
    },
    {
      "epoch": 19.609991235758105,
      "grad_norm": 8.869121551513672,
      "learning_rate": 1.731668127373649e-05,
      "loss": 1.8418,
      "step": 89500
    },
    {
      "epoch": 19.71954425942156,
      "grad_norm": 3.9561755657196045,
      "learning_rate": 1.7134092900964068e-05,
      "loss": 1.8362,
      "step": 90000
    },
    {
      "epoch": 19.829097283085012,
      "grad_norm": 4.55015230178833,
      "learning_rate": 1.6951504528191646e-05,
      "loss": 1.8407,
      "step": 90500
    },
    {
      "epoch": 19.938650306748468,
      "grad_norm": 3.618755340576172,
      "learning_rate": 1.6768916155419224e-05,
      "loss": 1.8293,
      "step": 91000
    },
    {
      "epoch": 20.04820333041192,
      "grad_norm": 7.360813140869141,
      "learning_rate": 1.6586327782646803e-05,
      "loss": 1.8356,
      "step": 91500
    },
    {
      "epoch": 20.15775635407537,
      "grad_norm": 8.085176467895508,
      "learning_rate": 1.640373940987438e-05,
      "loss": 1.8344,
      "step": 92000
    },
    {
      "epoch": 20.267309377738826,
      "grad_norm": 6.8371381759643555,
      "learning_rate": 1.622115103710196e-05,
      "loss": 1.8371,
      "step": 92500
    },
    {
      "epoch": 20.376862401402278,
      "grad_norm": 4.0792622566223145,
      "learning_rate": 1.603856266432954e-05,
      "loss": 1.8253,
      "step": 93000
    },
    {
      "epoch": 20.486415425065733,
      "grad_norm": 5.936682224273682,
      "learning_rate": 1.5855974291557117e-05,
      "loss": 1.8372,
      "step": 93500
    },
    {
      "epoch": 20.595968448729185,
      "grad_norm": 5.312279224395752,
      "learning_rate": 1.5673385918784692e-05,
      "loss": 1.8407,
      "step": 94000
    },
    {
      "epoch": 20.70552147239264,
      "grad_norm": 4.063791751861572,
      "learning_rate": 1.549079754601227e-05,
      "loss": 1.8441,
      "step": 94500
    },
    {
      "epoch": 20.81507449605609,
      "grad_norm": 8.692014694213867,
      "learning_rate": 1.530820917323985e-05,
      "loss": 1.8373,
      "step": 95000
    },
    {
      "epoch": 20.924627519719543,
      "grad_norm": 6.54412841796875,
      "learning_rate": 1.5125620800467427e-05,
      "loss": 1.8342,
      "step": 95500
    },
    {
      "epoch": 21.034180543383,
      "grad_norm": 6.93998908996582,
      "learning_rate": 1.4943032427695006e-05,
      "loss": 1.8406,
      "step": 96000
    },
    {
      "epoch": 21.14373356704645,
      "grad_norm": 5.388373851776123,
      "learning_rate": 1.4760444054922584e-05,
      "loss": 1.8306,
      "step": 96500
    },
    {
      "epoch": 21.253286590709905,
      "grad_norm": 7.014659881591797,
      "learning_rate": 1.4577855682150163e-05,
      "loss": 1.8294,
      "step": 97000
    },
    {
      "epoch": 21.362839614373357,
      "grad_norm": 6.007256031036377,
      "learning_rate": 1.4395267309377738e-05,
      "loss": 1.8298,
      "step": 97500
    },
    {
      "epoch": 21.47239263803681,
      "grad_norm": 4.417320728302002,
      "learning_rate": 1.4212678936605318e-05,
      "loss": 1.843,
      "step": 98000
    },
    {
      "epoch": 21.581945661700264,
      "grad_norm": 7.989488124847412,
      "learning_rate": 1.4030090563832896e-05,
      "loss": 1.8426,
      "step": 98500
    },
    {
      "epoch": 21.691498685363715,
      "grad_norm": 7.447692394256592,
      "learning_rate": 1.3847502191060475e-05,
      "loss": 1.8352,
      "step": 99000
    },
    {
      "epoch": 21.80105170902717,
      "grad_norm": 4.286766052246094,
      "learning_rate": 1.3664913818288053e-05,
      "loss": 1.8372,
      "step": 99500
    },
    {
      "epoch": 21.910604732690622,
      "grad_norm": 2.6775639057159424,
      "learning_rate": 1.3482325445515632e-05,
      "loss": 1.8343,
      "step": 100000
    },
    {
      "epoch": 22.020157756354074,
      "grad_norm": 5.737872123718262,
      "learning_rate": 1.329973707274321e-05,
      "loss": 1.8378,
      "step": 100500
    },
    {
      "epoch": 22.12971078001753,
      "grad_norm": 6.155948638916016,
      "learning_rate": 1.3117148699970785e-05,
      "loss": 1.8453,
      "step": 101000
    },
    {
      "epoch": 22.23926380368098,
      "grad_norm": 3.4430041313171387,
      "learning_rate": 1.2934560327198364e-05,
      "loss": 1.8292,
      "step": 101500
    },
    {
      "epoch": 22.348816827344436,
      "grad_norm": 6.2235589027404785,
      "learning_rate": 1.2751971954425942e-05,
      "loss": 1.8367,
      "step": 102000
    },
    {
      "epoch": 22.458369851007888,
      "grad_norm": 5.963412284851074,
      "learning_rate": 1.256938358165352e-05,
      "loss": 1.8301,
      "step": 102500
    },
    {
      "epoch": 22.56792287467134,
      "grad_norm": 5.446800231933594,
      "learning_rate": 1.2386795208881099e-05,
      "loss": 1.8418,
      "step": 103000
    },
    {
      "epoch": 22.677475898334794,
      "grad_norm": 6.384434700012207,
      "learning_rate": 1.2204206836108678e-05,
      "loss": 1.8374,
      "step": 103500
    },
    {
      "epoch": 22.787028921998246,
      "grad_norm": 6.4966583251953125,
      "learning_rate": 1.2021618463336256e-05,
      "loss": 1.8352,
      "step": 104000
    },
    {
      "epoch": 22.8965819456617,
      "grad_norm": 3.355464220046997,
      "learning_rate": 1.1839030090563834e-05,
      "loss": 1.834,
      "step": 104500
    },
    {
      "epoch": 23.006134969325153,
      "grad_norm": 10.083696365356445,
      "learning_rate": 1.1656441717791411e-05,
      "loss": 1.832,
      "step": 105000
    },
    {
      "epoch": 23.115687992988608,
      "grad_norm": 5.203965663909912,
      "learning_rate": 1.147385334501899e-05,
      "loss": 1.8342,
      "step": 105500
    },
    {
      "epoch": 23.22524101665206,
      "grad_norm": 6.337644100189209,
      "learning_rate": 1.1291264972246568e-05,
      "loss": 1.8337,
      "step": 106000
    },
    {
      "epoch": 23.33479404031551,
      "grad_norm": 4.303267955780029,
      "learning_rate": 1.1108676599474147e-05,
      "loss": 1.8319,
      "step": 106500
    },
    {
      "epoch": 23.444347063978967,
      "grad_norm": 2.9699411392211914,
      "learning_rate": 1.0926088226701723e-05,
      "loss": 1.8392,
      "step": 107000
    },
    {
      "epoch": 23.55390008764242,
      "grad_norm": 4.341555118560791,
      "learning_rate": 1.0743499853929302e-05,
      "loss": 1.8421,
      "step": 107500
    },
    {
      "epoch": 23.663453111305873,
      "grad_norm": 8.330113410949707,
      "learning_rate": 1.056091148115688e-05,
      "loss": 1.8365,
      "step": 108000
    },
    {
      "epoch": 23.773006134969325,
      "grad_norm": 4.73487663269043,
      "learning_rate": 1.0378323108384459e-05,
      "loss": 1.8366,
      "step": 108500
    },
    {
      "epoch": 23.882559158632777,
      "grad_norm": 5.100875377655029,
      "learning_rate": 1.0195734735612037e-05,
      "loss": 1.8362,
      "step": 109000
    },
    {
      "epoch": 23.992112182296232,
      "grad_norm": 7.1266374588012695,
      "learning_rate": 1.0013146362839616e-05,
      "loss": 1.8301,
      "step": 109500
    },
    {
      "epoch": 24.101665205959684,
      "grad_norm": 7.787258625030518,
      "learning_rate": 9.830557990067194e-06,
      "loss": 1.8298,
      "step": 110000
    },
    {
      "epoch": 24.21121822962314,
      "grad_norm": 7.978237152099609,
      "learning_rate": 9.647969617294771e-06,
      "loss": 1.8322,
      "step": 110500
    },
    {
      "epoch": 24.32077125328659,
      "grad_norm": 4.659352779388428,
      "learning_rate": 9.46538124452235e-06,
      "loss": 1.8263,
      "step": 111000
    },
    {
      "epoch": 24.430324276950042,
      "grad_norm": 7.813002109527588,
      "learning_rate": 9.282792871749928e-06,
      "loss": 1.843,
      "step": 111500
    },
    {
      "epoch": 24.539877300613497,
      "grad_norm": 2.8111023902893066,
      "learning_rate": 9.100204498977506e-06,
      "loss": 1.8392,
      "step": 112000
    },
    {
      "epoch": 24.64943032427695,
      "grad_norm": 6.839910507202148,
      "learning_rate": 8.917616126205083e-06,
      "loss": 1.8293,
      "step": 112500
    },
    {
      "epoch": 24.758983347940404,
      "grad_norm": 3.7264158725738525,
      "learning_rate": 8.735027753432661e-06,
      "loss": 1.8398,
      "step": 113000
    },
    {
      "epoch": 24.868536371603856,
      "grad_norm": 4.92772912979126,
      "learning_rate": 8.55243938066024e-06,
      "loss": 1.8289,
      "step": 113500
    },
    {
      "epoch": 24.97808939526731,
      "grad_norm": 3.8665812015533447,
      "learning_rate": 8.369851007887818e-06,
      "loss": 1.8379,
      "step": 114000
    },
    {
      "epoch": 25.087642418930763,
      "grad_norm": 5.045696258544922,
      "learning_rate": 8.187262635115397e-06,
      "loss": 1.8373,
      "step": 114500
    },
    {
      "epoch": 25.197195442594214,
      "grad_norm": 3.5917270183563232,
      "learning_rate": 8.004674262342975e-06,
      "loss": 1.8292,
      "step": 115000
    },
    {
      "epoch": 25.30674846625767,
      "grad_norm": 3.766451597213745,
      "learning_rate": 7.822085889570554e-06,
      "loss": 1.8287,
      "step": 115500
    },
    {
      "epoch": 25.41630148992112,
      "grad_norm": 5.566201686859131,
      "learning_rate": 7.63949751679813e-06,
      "loss": 1.8351,
      "step": 116000
    },
    {
      "epoch": 25.525854513584576,
      "grad_norm": 4.0350494384765625,
      "learning_rate": 7.456909144025709e-06,
      "loss": 1.8328,
      "step": 116500
    },
    {
      "epoch": 25.635407537248028,
      "grad_norm": 4.776090145111084,
      "learning_rate": 7.2743207712532874e-06,
      "loss": 1.838,
      "step": 117000
    },
    {
      "epoch": 25.74496056091148,
      "grad_norm": 3.2931969165802,
      "learning_rate": 7.091732398480864e-06,
      "loss": 1.8389,
      "step": 117500
    },
    {
      "epoch": 25.854513584574935,
      "grad_norm": 6.023141860961914,
      "learning_rate": 6.909144025708443e-06,
      "loss": 1.8359,
      "step": 118000
    },
    {
      "epoch": 25.964066608238387,
      "grad_norm": 5.727145195007324,
      "learning_rate": 6.726555652936022e-06,
      "loss": 1.8376,
      "step": 118500
    },
    {
      "epoch": 26.073619631901842,
      "grad_norm": 7.881748199462891,
      "learning_rate": 6.5439672801636004e-06,
      "loss": 1.8288,
      "step": 119000
    },
    {
      "epoch": 26.183172655565293,
      "grad_norm": 4.620242595672607,
      "learning_rate": 6.361378907391177e-06,
      "loss": 1.8428,
      "step": 119500
    },
    {
      "epoch": 26.292725679228745,
      "grad_norm": 5.007565498352051,
      "learning_rate": 6.178790534618756e-06,
      "loss": 1.8395,
      "step": 120000
    },
    {
      "epoch": 26.4022787028922,
      "grad_norm": 4.473686218261719,
      "learning_rate": 5.996202161846333e-06,
      "loss": 1.8263,
      "step": 120500
    },
    {
      "epoch": 26.511831726555652,
      "grad_norm": 5.463870048522949,
      "learning_rate": 5.813613789073913e-06,
      "loss": 1.836,
      "step": 121000
    },
    {
      "epoch": 26.621384750219107,
      "grad_norm": 6.2659502029418945,
      "learning_rate": 5.63102541630149e-06,
      "loss": 1.8377,
      "step": 121500
    },
    {
      "epoch": 26.73093777388256,
      "grad_norm": 4.8414387702941895,
      "learning_rate": 5.448437043529069e-06,
      "loss": 1.836,
      "step": 122000
    },
    {
      "epoch": 26.84049079754601,
      "grad_norm": 6.436367034912109,
      "learning_rate": 5.265848670756646e-06,
      "loss": 1.835,
      "step": 122500
    },
    {
      "epoch": 26.950043821209466,
      "grad_norm": 5.01788330078125,
      "learning_rate": 5.083260297984225e-06,
      "loss": 1.831,
      "step": 123000
    },
    {
      "epoch": 27.059596844872917,
      "grad_norm": 3.8335795402526855,
      "learning_rate": 4.900671925211802e-06,
      "loss": 1.8348,
      "step": 123500
    },
    {
      "epoch": 27.169149868536373,
      "grad_norm": 4.496392726898193,
      "learning_rate": 4.718083552439381e-06,
      "loss": 1.832,
      "step": 124000
    },
    {
      "epoch": 27.278702892199824,
      "grad_norm": 3.161478042602539,
      "learning_rate": 4.535495179666959e-06,
      "loss": 1.8313,
      "step": 124500
    },
    {
      "epoch": 27.38825591586328,
      "grad_norm": 4.295859336853027,
      "learning_rate": 4.352906806894537e-06,
      "loss": 1.8367,
      "step": 125000
    },
    {
      "epoch": 27.49780893952673,
      "grad_norm": 3.9755847454071045,
      "learning_rate": 4.170318434122115e-06,
      "loss": 1.8307,
      "step": 125500
    },
    {
      "epoch": 27.607361963190183,
      "grad_norm": 4.304571628570557,
      "learning_rate": 3.987730061349693e-06,
      "loss": 1.8376,
      "step": 126000
    },
    {
      "epoch": 27.716914986853638,
      "grad_norm": 4.287022590637207,
      "learning_rate": 3.805141688577272e-06,
      "loss": 1.836,
      "step": 126500
    },
    {
      "epoch": 27.82646801051709,
      "grad_norm": 5.24130916595459,
      "learning_rate": 3.62255331580485e-06,
      "loss": 1.8391,
      "step": 127000
    },
    {
      "epoch": 27.936021034180545,
      "grad_norm": 8.999889373779297,
      "learning_rate": 3.4399649430324275e-06,
      "loss": 1.8317,
      "step": 127500
    },
    {
      "epoch": 28.045574057843996,
      "grad_norm": 5.3946661949157715,
      "learning_rate": 3.257376570260006e-06,
      "loss": 1.8317,
      "step": 128000
    },
    {
      "epoch": 28.155127081507448,
      "grad_norm": 6.499451637268066,
      "learning_rate": 3.0747881974875844e-06,
      "loss": 1.8372,
      "step": 128500
    },
    {
      "epoch": 28.264680105170903,
      "grad_norm": 3.927987575531006,
      "learning_rate": 2.8921998247151624e-06,
      "loss": 1.8317,
      "step": 129000
    },
    {
      "epoch": 28.374233128834355,
      "grad_norm": 4.3377180099487305,
      "learning_rate": 2.7096114519427405e-06,
      "loss": 1.8285,
      "step": 129500
    },
    {
      "epoch": 28.48378615249781,
      "grad_norm": 4.1433820724487305,
      "learning_rate": 2.5270230791703185e-06,
      "loss": 1.831,
      "step": 130000
    },
    {
      "epoch": 28.593339176161262,
      "grad_norm": 5.973471164703369,
      "learning_rate": 2.3444347063978965e-06,
      "loss": 1.8277,
      "step": 130500
    },
    {
      "epoch": 28.702892199824714,
      "grad_norm": 4.193670272827148,
      "learning_rate": 2.161846333625475e-06,
      "loss": 1.8449,
      "step": 131000
    },
    {
      "epoch": 28.81244522348817,
      "grad_norm": 8.870223999023438,
      "learning_rate": 1.979257960853053e-06,
      "loss": 1.8381,
      "step": 131500
    },
    {
      "epoch": 28.92199824715162,
      "grad_norm": 5.034728050231934,
      "learning_rate": 1.7966695880806313e-06,
      "loss": 1.8329,
      "step": 132000
    },
    {
      "epoch": 29.031551270815076,
      "grad_norm": 2.47426700592041,
      "learning_rate": 1.6140812153082093e-06,
      "loss": 1.834,
      "step": 132500
    },
    {
      "epoch": 29.141104294478527,
      "grad_norm": 5.388508319854736,
      "learning_rate": 1.4314928425357874e-06,
      "loss": 1.8426,
      "step": 133000
    },
    {
      "epoch": 29.250657318141982,
      "grad_norm": 5.972341537475586,
      "learning_rate": 1.2489044697633656e-06,
      "loss": 1.8258,
      "step": 133500
    },
    {
      "epoch": 29.360210341805434,
      "grad_norm": 4.837533473968506,
      "learning_rate": 1.0663160969909436e-06,
      "loss": 1.827,
      "step": 134000
    },
    {
      "epoch": 29.469763365468886,
      "grad_norm": 3.958587169647217,
      "learning_rate": 8.837277242185218e-07,
      "loss": 1.8304,
      "step": 134500
    },
    {
      "epoch": 29.57931638913234,
      "grad_norm": 7.142270088195801,
      "learning_rate": 7.011393514460999e-07,
      "loss": 1.8368,
      "step": 135000
    },
    {
      "epoch": 29.688869412795793,
      "grad_norm": 6.725022315979004,
      "learning_rate": 5.185509786736781e-07,
      "loss": 1.8307,
      "step": 135500
    },
    {
      "epoch": 29.798422436459248,
      "grad_norm": 6.592309474945068,
      "learning_rate": 3.359626059012562e-07,
      "loss": 1.8374,
      "step": 136000
    },
    {
      "epoch": 29.9079754601227,
      "grad_norm": 3.445159673690796,
      "learning_rate": 1.5337423312883438e-07,
      "loss": 1.8367,
      "step": 136500
    },
    {
      "epoch": 30.0,
      "step": 136920,
      "total_flos": 5.76325909714176e+17,
      "train_loss": 1.8346385544585513,
      "train_runtime": 50208.9409,
      "train_samples_per_second": 43.624,
      "train_steps_per_second": 2.727
    }
  ],
  "logging_steps": 500,
  "max_steps": 136920,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 30,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.76325909714176e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
